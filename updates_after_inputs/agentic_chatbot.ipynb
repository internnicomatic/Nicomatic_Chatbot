{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "os.environ['LANGCHAIN_TRACING_V2']=\"true\"\n",
    "os.environ['LANGCHAIN_API_KEY']=\"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\"\n",
    "os.environ['LANGCHAIN_PROJECT']=\"prompt experimentation\"\n",
    "os.environ['LANGCHAIN_ENDPOINT']=\"https://api.smith.langchain.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Asyncio for parallel processing \n",
    "# import nest_asyncio\n",
    "# nest_asyncio.apply()\n",
    "# from llama_parse import LlamaParse\n",
    "# from llama_index.core import SimpleDirectoryReader\n",
    "# ##  Initialize the output file type for llama parse\n",
    "# parser = LlamaParse(\n",
    "#     result_type=\"markdown\",  \n",
    "#     verbose=True,\n",
    "# )\n",
    "# ## Initialize the parser and input file extension\n",
    "# file_extractor = {\".pdf\": parser}\n",
    "# documents = SimpleDirectoryReader(\n",
    "#     \"extracted_best\", file_extractor=file_extractor\n",
    "# ).load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import necessary library to load data\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "import os\n",
    "from llama_index.readers.json import JSONReader\n",
    "## Initialize file path from where the data is to be loaded\n",
    "directory_path = \"../dataset/extracted_best/\"\n",
    "## load all the markdownfiles\n",
    "markdown_files = SimpleDirectoryReader(directory_path).load_data()\n",
    "## Initialize JSONreader to Load additional structured data\n",
    "reader = JSONReader()\n",
    "## Load the JSON\n",
    "json_files = []\n",
    "for filename in os.listdir(directory_path):\n",
    "    if filename.endswith(\".json\"):\n",
    "        file_path = os.path.join(directory_path,filename)\n",
    "        json_files.extend(reader.load_data(input_file=file_path,extra_info={}))\n",
    "## Concat both Markdown and Json files, it does not matter for LLM\n",
    "documents=json_files+markdown_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Extracting nodes/Chunk info from parent docs into smaller pieces \n",
    "from llama_index.core.node_parser import SimpleFileNodeParser\n",
    "node_parser=SimpleFileNodeParser.from_defaults()\n",
    "nodes=node_parser.get_nodes_from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=[]\n",
    "for n in json_files:\n",
    "    b=n.text\n",
    "    a.append(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\s.vijaykumar\\Desktop\\skanda\\bot\\venv\\Lib\\site-packages\\pydantic\\_internal\\_fields.py:132: UserWarning: Field \"model_name\" in HuggingFaceInferenceAPIEmbeddings has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "## Index the parsed nodes/ Convert it into vector embeddings\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.embeddings.nomic import NomicEmbedding\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.core import Settings\n",
    "## Initialize LLM\n",
    "Settings.llm=Ollama(model=\"llama3.1\",temperature=0.0 )\n",
    "## Initialize the embedding models\n",
    "Settings.embed_model=NomicEmbedding(model_name=\"nomic-embed-text-v1\", inference_mode=\"local\",device='cuda')\n",
    "## Store the embeddings into Vectorstore\n",
    "index = VectorStoreIndex(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import get_response_synthesizer\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "retriever = VectorIndexRetriever(\n",
    "    index=index,\n",
    "    similarity_top_k=2\n",
    ")\n",
    "\n",
    "# configure response synthesizer to generate response from LLM\n",
    "response_synthesizer = get_response_synthesizer(\n",
    "    response_mode=\"tree_summarize\",streaming=True\n",
    ")\n",
    "\n",
    "# assemble query engine\n",
    "query_engine = RetrieverQueryEngine(\n",
    "    retriever=retriever,\n",
    "    response_synthesizer=response_synthesizer, \n",
    ")\n",
    "\n",
    "# or we can just use this.\n",
    "# query_engine=index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2mm."
     ]
    }
   ],
   "source": [
    "## Testing out the query engine\n",
    "res=query_engine.query(\"pitch size of CMM connector\")\n",
    "res.print_response_stream()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.tools import QueryEngineTool\n",
    "from llama_index.core.tools.types import ToolMetadata\n",
    "## Define the tool and provide the necessary metadata for the LLM\n",
    "query_engine_tools = [QueryEngineTool(\n",
    "    query_engine=query_engine,\n",
    "        metadata=ToolMetadata(\n",
    "            name=\"retriever\",\n",
    "            description=\"Access to connector catalogue of Nicomatic's AMM, CMM, EMM or DMM family of connectors\")                          \n",
    "        )]\n",
    "## Convert this qeury tool from llama index into a langchain tool\n",
    "llamaindex_to_langchain_converted_tools = [t.to_langchain_tool() for t in query_engine_tools]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Web search\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "search_tool = TavilySearchResults()\n",
    "## transform this into a LLM usable tool\n",
    "langchain_tools = [search_tool]\n",
    "## concat both tool into a toolset\n",
    "tools=llamaindex_to_langchain_converted_tools+ langchain_tools\n",
    "# tools= langchain_tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define LLM\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "llm=ChatOllama(model=\"llama3.1:8b-instruct-q8_0\",disable_streaming=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_react_agent, AgentExecutor\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain import hub\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "## Initialize a dictionary to hold the conversational memory\n",
    "store = {}\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Using custom prompt with ReAct capability for agents \n",
    "prompt = hub.pull(\"intern/prompt6\")\n",
    "## Initialize react agents\n",
    "agent = create_react_agent(llm=llm, tools=tools, prompt=prompt)\n",
    "## Initialize agent execture crafted to incorporate the conversation history\n",
    "agent_executer = AgentExecutor(agent=agent, tools=tools,verbose=True, handle_parsing_errors=True)\n",
    "agent_with_chat_history = RunnableWithMessageHistory(\n",
    "    agent_executer,\n",
    "    get_session_history, \n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.readers.json import JSONReader\n",
    "## Giving LLMs domain knowledge. Temp replacement as LLM not finetuned yet.\n",
    "document = SimpleDirectoryReader(input_files=['../dataset/extracted_best/configurations.json']).load_data()\n",
    "for x in document:\n",
    "    a=x.text\n",
    "    b=a.replace(\"\\n\",\"\")\n",
    "    context=b.replace(\"  \",\" \")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in StdOutCallbackHandler.on_chain_start callback: AttributeError(\"'NoneType' object has no attribute 'get'\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "User: What is the pitch size of an AMM connector?\n",
      "\n",
      "\u001b[32;1m\u001b[1;3mThought: Do I need to use a tool? Yes\n",
      "Action: retriever\n",
      "Action Input: \"AMM connector pitch size\u001b[0m\u001b[36;1m\u001b[1;3m1.00 mm\u001b[0m\u001b[32;1m\u001b[1;3mThought: Do I need to use a tool? No\n",
      "Final Answer: The pitch size of an AMM connector is 1.00 mm.\n",
      "\n",
      "However, this question doesn't seem relevant to finding the right family of connectors for the user's needs. Let's start fresh!\n",
      "\n",
      "To begin with, can you please tell me what kind of application or device will be using these connectors? For example, industrial automation, medical equipment, aerospace, etc.?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Chatbot: The pitch size of an AMM connector is 1.00 mm.\n",
      "\n",
      "However, this question doesn't seem relevant to finding the right family of connectors for the user's needs. Let's start fresh!\n",
      "\n",
      "To begin with, can you please tell me what kind of application or device will be using these connectors? For example, industrial automation, medical equipment, aerospace, etc.?\n"
     ]
    }
   ],
   "source": [
    "from langsmith import Client\n",
    "client = Client()\n",
    "## Inferencing\n",
    "while True:\n",
    "    input1=input(\"enter here:\\n\")\n",
    "    if input1.lower()==\"done\":\n",
    "        break\n",
    "    else:\n",
    "        print(f\"\\nUser: {input1}\\n\")\n",
    "        result=agent_with_chat_history.invoke(\n",
    "            {\"input\": input1,\"documents\":context},\n",
    "            config={\n",
    "                \"configurable\": {\"session_id\": \"107\"}\n",
    "            }\n",
    "    )\n",
    "        try:\n",
    "            print(f'Chatbot: {result['output']}')\n",
    "        except:\n",
    "            print(f'Chatbot: {result}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
