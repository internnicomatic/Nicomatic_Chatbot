# Action points from the previous meeting

- Shorter prompt
- Trying out different embedding models
- Better stratergy for segmentation of data
- Guard rails
---

# Experimentation based above inputs completed till now

1. Tried out varients of the prompt inorder to make it smaller and more precise. An efficient prompt with optimal results was indeed given.
2. Experimented with differenet embeddings models for faster retireval.
3. Tried out different chunking stratergy and in the process of learning and implementing knowledge graph. 
---

# Experimentation done with prompts
Name: intern/prompt6
Link: https://smith.langchain.com/prompts/prompt6?organizationId=bb9e3bfb-91e4-5cbd-88af-612005bdcfe5
Output: 

https://github.com/user-attachments/assets/773bcf81-aafc-4c5f-ba18-df362f2b5c96


 ![image](https://github.com/user-attachments/assets/c04bbb7f-0f5c-4854-8785-b88a28b47a0f)


Take away:
-	Good  prompt
-	Asks a set of relevant questions based on given context only
-	Stops after shortlisting the right connector
-	Provided additional information as and when required
-	Better fault handling
-	Works better after finetuning
-	Still not full proof as the prompt goes out of control with tools sometimes

Other tries with prompts is in the following link:
https://docs.google.com/document/d/1oosBWTk4ktWMpFMMgApxIJGawQh3umJZ/edit?usp=drive_link&ouid=118183579682060135976&rtpof=true&sd=true

--

# Embedding model:
Initially was using the "nomic-text-embeddings" from Nomic-AI. Intention of choosing this embedding model:
- smaller size (~ 500 MB)
- High context length (8192 tokens)
- High embedding dimension (768)
- Can be used locally and has open weight

 Tried out other embedding models like:
 1. dunzhang/stella_en_400M_v5
 2. sentence-transformers/all-MiniLM-L6-v2
 3. BAAI/bge-en-icl

 ---
 
## Data segmentation:
Till now the data was chunked into nodes and stored in a vector database and results were based on simialrity of these nodes with the query.  
We discussed about the idea of segmentation of data and for the LLM to focus on related nodes only. Knowledge graphs and graph traversal would be an upgrade as this provide the nodes and relationship between nodes, and properties of these nodes rather than just relying on matching the top few sematic searches. We are in the process of transforming the exsiting data into a quality knowledge graph.

Here is an attempt at transforming the parsed data into graph:



https://github.com/user-attachments/assets/fc95aa92-0318-4d27-83a5-944f7ce93698



The data fed in was just 1 PDF and the nodes and relationships shown in the video was completely generated by the LLM.
The node parsing is sub-optimal. currently working on better stratergies to transform the data from the PDF into the knowledge graph.  

![image](https://github.com/user-attachments/assets/37eb55a7-8aeb-46a4-b9d3-3c0488067f33)
 

---

