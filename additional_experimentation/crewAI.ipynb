{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load Markdown files parsed from llama parse\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "markdown_files = SimpleDirectoryReader(\"dataset/extracted_best/\").load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load structured data (JSON format)\n",
    "import os\n",
    "from llama_index.readers.json import JSONReader\n",
    "reader = JSONReader()\n",
    "directory_path = \"dataset/extracted_best/\"\n",
    "json_files = []\n",
    "for filename in os.listdir(directory_path):\n",
    "    if filename.endswith(\".json\"):\n",
    "        file_path = os.path.join(directory_path,filename)\n",
    "        json_files.extend(reader.load_data(input_file=file_path,extra_info={}))\n",
    "## Concat both Markdown and Json files, it does not matter as for LLM data is no different\n",
    "documents=markdown_files+json_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Extracting nodes/Chunk info from parent docs into smaller pieces \n",
    "from llama_index.core.node_parser import MarkdownNodeParser\n",
    "node_parser=MarkdownNodeParser.from_defaults()\n",
    "nodes=node_parser.get_nodes_from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Index the parsed nodes/ Convert it into vector embeddings\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.embeddings.nomic import NomicEmbedding\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.core import Settings\n",
    "Settings.llm=Ollama(model=\"llama3\",temperature=0)\n",
    "Settings.embed_model=NomicEmbedding(model_name=\"nomic-embed-text-v1\", inference_mode=\"local\")\n",
    "## Store the embeddings into Vectorstore\n",
    "index = VectorStoreIndex(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create query engine for retrieval of data from vector db\n",
    "query_engine = index.as_query_engine()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define LLM, here using Ollama to run local model\n",
    "llm=Ollama(model=\"llama3.1\",temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Convert the query engine previously defined into a tool for agents to use.\n",
    "from llama_index.core.tools import QueryEngineTool,ToolMetadata\n",
    "retriever = QueryEngineTool.from_defaults(query_engine,name=\"retriever\",\n",
    "                                          description=\"A RAG engine with information about AMM, CMM, DMM and EMM connectors of Nicomatic manufacturers.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.agent import ReActAgent\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from langchain import hub\n",
    "## System prompt to make the LLM think and give response\n",
    "prompt = hub.pull(\"hwchase17/react\")\n",
    "llm=Ollama(model=\"llama3.1\",temperature=0)\n",
    "## Define a React agent to test the tool\n",
    "agent = ReActAgent.from_tools([retriever],llm=llm,verbose=True,system_prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Running step 59be6eb8-e681-4452-b738-8e91b4bcb64c. Step input: What is the length and height of CMM 320 Series female with 39 contacts?\n",
      "\u001b[1;3;38;5;200mThought: The current language of the user is: English. I need to use a tool to help me answer the question.\n",
      "Action: retriever\n",
      "Action Input: {'properties': AttributedDict([('input', 'CMM 320 Series female with 39 contacts')])}\n",
      "\u001b[0m\u001b[1;3;34mObservation: According to the provided data, a CMM 320 Series female connector has 39 contacts. The corresponding length is 39 mm and height is 7.7 mm.\n",
      "\u001b[0m> Running step 7c66d2ca-8523-4053-a84e-b508eb00fb44. Step input: None\n",
      "\u001b[1;3;38;5;200mThought: I can answer without using any more tools. I'll use the user's language to answer\n",
      "Answer: A CMM 320 Series female connector with 39 contacts has a length of 39 mm and a height of 7.7 mm.\n",
      "\u001b[0mA CMM 320 Series female connector with 39 contacts has a length of 39 mm and a height of 7.7 mm.\n"
     ]
    }
   ],
   "source": [
    "## Inference the agent\n",
    "response = agent.query(\"What is the length and height of CMM 320 Series female with 39 contacts?\")\n",
    "print(response)\n",
    "## This output is for react agents without memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CrewAI for Agent Orchastration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import Ollama\n",
    "## Initialize LLM \n",
    "model=Ollama(model='llama3.1',temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import  Tool\n",
    "from crewai_tools import LlamaIndexTool\n",
    "## Define Query tool\n",
    "query_tool = LlamaIndexTool.from_query_engine(\n",
    "    query_engine,\n",
    "    name=\"retriever\", description=\"Use this tool to access catalogue of Nicomatic's range of AMM, CMM, DMM and EMM connectors.\"\n",
    ")\n",
    "## Websearch tool\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "web_search_tool = TavilySearchResults()\n",
    "search_tool = Tool(\n",
    "    name=\"web search tool\",\n",
    "    description=\"A tool to access internet through web search.\",\n",
    "    func=web_search_tool.run,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from crewai import Agent, Task, Crew, Process\n",
    "from crewai_tools import LlamaIndexTool\n",
    "## Define an Agent and give it tools\n",
    "tavily = Agent(\n",
    "    role=\"chatbot\",\n",
    "    goal=\"Answer user query accurately\",\n",
    "    backstory=\"\"\"You are an AI chatbot.\n",
    "                You have knowledge on Nicomatic's AMM,CMM,DMM and EMM connectors.\n",
    "                You have access for a tool to search the internet and tool to browse through connector catalogue.\n",
    "                Your final answer should be in casual english.\"\"\",\n",
    "    verbose=False,\n",
    "    allow_delegation=False,\n",
    "    memory=True,\n",
    "    tools=[search_tool,query_tool],\n",
    "    llm=model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define a task for the crew\n",
    "task1 = Task(\n",
    "    description=  \"\"\"\n",
    "        You are an AI chat bot. You need to answer all the user queries accurately.\n",
    "        Breakdown the user query into components of simpler question and then try to answer it.\n",
    "        If you do not find answers in the catalauge try to use \n",
    "        Your final answer consise and should be .\n",
    "\n",
    "        Here is the customer's question:\n",
    "        {customer_question}\n",
    "        \"\"\",\n",
    "    expected_output=\"Accurate answers\",\n",
    "    agent=tavily,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create crew\n",
    "crew = Crew(\n",
    "    agents=[tavily],\n",
    "    tasks=[task1],\n",
    "    verbose=True,  \n",
    "    process = Process.sequential,\n",
    "    llm=model,\n",
    "    max_iterations=150,\n",
    "    max_execution_time=300,\n",
    "    memory=True,\n",
    "    embedder={\n",
    "            \"provider\": \"ollama\",\n",
    "            \"config\":{\n",
    "                \"model\": \"nomic-embed-text:latest\",\n",
    "                \"vector_dimension\": 1024,\n",
    "}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[95m [2024-09-11 10:11:26][DEBUG]: == Working Agent: chatbot\u001b[00m\n",
      "\u001b[1m\u001b[95m [2024-09-11 10:11:26][INFO]: == Starting Task: \n",
      "        You are an AI chat bot. You need to answer all the user queries accurately.\n",
      "        Breakdown the user query into components of simpler question and then try to answer it.\n",
      "        If you do not find answers in the catalauge try to use \n",
      "        Your final answer consise and should be .\n",
      "\n",
      "        Here is the customer's question:\n",
      "        AMM pitch size\n",
      "        \u001b[00m\n",
      "\u001b[1m\u001b[92m [2024-09-11 10:11:58][DEBUG]: == [chatbot] Task output: 1.00 mm pitch.\n",
      "\n",
      "\u001b[00m\n",
      "1.00 mm pitch.\n",
      "Au revoir!\n"
     ]
    }
   ],
   "source": [
    "## Infer the crew\n",
    "while True:\n",
    "    customer_question = input(\"Enter your question (type 'done' to exit): \")\n",
    "    if customer_question.lower() == \"done\":\n",
    "        print(\"Au revoir!\")\n",
    "        break\n",
    "    else:\n",
    "        result = crew.kickoff(inputs={\"customer_question\": customer_question})\n",
    "        print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.00 mm pitch.\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
